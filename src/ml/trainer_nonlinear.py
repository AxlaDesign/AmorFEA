'''Train the neural network to be a powerful PDE solver, where physical laws have been built in
Linear case
'''

import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import numpy as np
import os
import collections
from .trainer import Trainer
from .models import LinearRegressor, MLP
from .generator import get_graph_attributes
from ..graph.domain import GraphMSHRTrapezoid
from .. import arguments
from ..graph.visualization import *


class TrainerNonlinear(Trainer):
    def __init__(self, args):
        super(TrainerNonlinear, self).__init__(args)
        self.graph = GraphMSHRTrapezoid(self.args)
        self.args.input_size = self.graph.num_vertices

    def loss_function(self, x_control, x_state):
        # loss function is defined so that PDE is satisfied
        # x_control should be torch tensor with shape (batch, input_size)
        # x_state should be torch tensor with shape (batch, input_size)
        assert(x_control.shape == x_state.shape and len(x_control.shape) == 2)
        x_state = x_state.unsqueeze(2)
        x_control = x_control.unsqueeze(2)
        source = self.source.unsqueeze(1)

        grad_x1 = self.batch_mm(self.grad_x1_sp, x_state)
        grad_x2 = self.batch_mm(self.grad_x2_sp, x_state)

        density = 0.5*x_control*(grad_x1**2 + grad_x2**2) - x_state*source
        loss = (density.squeeze() * self.weight_area).sum()
        return loss

    def initialization(self):
        # Can be much more general
        self.gradient_x1_operator = torch.tensor(self.graph.gradient_x1).float()
        self.gradient_x2_operator = torch.tensor(self.graph.gradient_x2).float()
        self.weight_area = torch.tensor(self.graph.weight_area).float()
        self.source = torch.ones(self.graph.num_vertices)
        self.grad_x1_sp = self.gradient_x1_operator.to_sparse()
        self.grad_x2_sp = self.gradient_x2_operator.to_sparse()

        bc_flag_1 = torch.tensor(self.graph.boundary_flags_list[0]).float()
        bc_value_1 = 2.*bc_flag_1
        bc_flag_2 = torch.tensor(self.graph.boundary_flags_list[1]).float()
        bc_value_2 = 1.*bc_flag_2
        bc_value = bc_value_1 + bc_value_2
        interior_flag = torch.ones(self.graph.num_vertices) - bc_flag_1 - bc_flag_2
        self.graph_info = [bc_value, interior_flag]


    def save_progress(self, np_data, L_inf, L_fro, milestone, counter):
        if L_inf < milestone[counter]:
            torch.save(self.model, self.args.root_path + '/' + self.args.model_path + '/model_' + str(counter))
            np.save(self.args.root_path + '/' + self.args.numpy_path + '/error_' + str(counter), np_data)
            counter += 1
        return counter

    def run(self):
        self.initialization()
 
        # Subject to change. Raw data generated by some distribution
        self.data_X = np.load(self.args.root_path + '/' + self.args.numpy_path + '/nonlinear/' 
                              +self.graph.name + '-Det-3000-' + str(self.graph.num_vertices) + '.npy')

        self.train_loader, self.test_loader = self.shuffle_data()
        self.model = MLP(self.args, self.graph_info)

        # nn.init.constant_(self.model.encoder.weight.data, 0.)
        # self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)
        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.5)

        milestone = [ 2**(-i) for i in range(-1, 11) ]

        for epoch in range(self.args.epochs):
            train_loss = self.train(epoch)
            test_loss = self.test(epoch)  
            print('\n')

        torch.save(self.model, self.args.root_path + '/nonlinear/' + self.args.model_path + '/model')

    def debug(self):
        self.initialization()

        # def exact_u(x1, x2):
        #     return x1**2 + 3*x2**2
        # sol = get_graph_attributes(exact_u, self.graph)
        # sol = torch.tensor(sol).float()
        # grad_x1 = torch.matmul(self.gradient_x1_operator, sol)
        # lap_x1 = torch.matmul(self.gradient_x1_operator, grad_x1)
        # grad_x2 = torch.matmul(self.gradient_x2_operator, sol)
        # lap_x2 = torch.matmul(self.gradient_x2_operator, grad_x2)
        # lap = lap_x1 + lap_x2

        def exact_u(x1, x2):
            return x1 + 3*x2
        sol = get_graph_attributes(exact_u, self.graph)
        sol = torch.tensor(sol).float()
        grad_x1 = torch.matmul(self.gradient_x1_operator, sol)
        grad_x2 = torch.matmul(self.gradient_x2_operator, sol)
        grad = grad_x1 + grad_x2

        print(grad)

        # scalar_field_2D(sol, self.graph)
        # plt.show()


if __name__ == "__main__":
    args = arguments.args
    trainer = TrainerNonlinear(args)
    trainer.run()