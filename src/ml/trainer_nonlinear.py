'''Train the neural network to be a powerful PDE solver, where physical laws have been built in
Linear case
'''

import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import numpy as np
import os
import collections
from .trainer import Trainer, batch_mat_vec
from .models import LinearRegressor, MLP, GCN, MixedNetwork
from .generator import get_graph_attributes
from ..graph.domain import GraphMSHRTrapezoid
from .. import arguments
from ..graph.visualization import *
from ..pde.poisson_trapezoid import PoissonTrapezoid


class TrainerNonlinear(Trainer):
    def __init__(self, args):
        super(TrainerNonlinear, self).__init__(args)
        self.graph = GraphMSHRTrapezoid(self.args)
        self.args.input_size = self.graph.num_vertices
        self.pde = PoissonTrapezoid(args)

    def loss_function(self, x_control, x_state):
        # loss function is defined so that PDE is satisfied
        # x_control should be torch tensor with shape (batch, input_size)
        # x_state should be torch tensor with shape (batch, input_size)

        if len(x_control.shape) == 1:
            x_control = x_control.unsqueeze(0)
        if len(x_state.shape) == 1:
            x_state = x_state.unsqueeze(0)

        assert(x_control.shape == x_state.shape and len(x_control.shape) == 2)
        grad_x1 = batch_mat_vec(self.grad_x1_sp, x_state)
        grad_x2 = batch_mat_vec(self.grad_x2_sp, x_state)

        density = 0.5*(grad_x1**2 + grad_x2**2) + 1*0.5*x_state**2 + 1*0.25*x_state**4 - x_state*x_control
        loss = (density * self.weight_area).sum()
        return loss

    def fem_ground_truth(self):
        num_extra_test = 100
        self.extra_test = self.test_X[:num_extra_test]
        fem_solution = []
        fem_energy = []
        for i in range(num_extra_test):
            self.pde.set_control_variable(self.extra_test[i])
            u = self.pde.solve_problem_variational_form()
            dof_data = u.vector()[:]
            vertex_data = np.array([dof_data[index] for index in self.pde.v_d])
            fem_solution.append(vertex_data)
            fem_energy.append(self.pde.energy(u))
        self.fem_solution = torch.Tensor(fem_solution).float()
        self.fem_energy = torch.Tensor(fem_energy).float()        

    def normalize_adj(self, A):
        size = A.shape[0]
        A = A + np.identity(size)

        self.A_debug = A

        D = np.array(A.sum(1))
        D = np.diag(D**(-0.5))
        A_normalized = np.matmul(np.matmul(D, A), D)
        A_normalized = torch.tensor(A_normalized).float()
        A_sp = A_normalized.to_sparse()
        return A_sp

    def initialization(self):
        # Can be much more general
        self.gradient_x1_operator = torch.tensor(self.graph.gradient_x1).float()
        self.gradient_x2_operator = torch.tensor(self.graph.gradient_x2).float()
        self.weight_area = torch.tensor(self.graph.weight_area).float()
        self.grad_x1_sp = self.gradient_x1_operator.to_sparse()
        self.grad_x2_sp = self.gradient_x2_operator.to_sparse()

        bc_flag_1 = torch.tensor(self.graph.boundary_flags_list[0]).float()
        bc_value_1 = 0.*bc_flag_1
        bc_flag_2 = torch.tensor(self.graph.boundary_flags_list[1]).float()
        bc_value_2 = 0.*bc_flag_2
        bc_value = bc_value_1 + bc_value_2
        interior_flag = torch.ones(self.graph.num_vertices) - bc_flag_1 - bc_flag_2
        
        # Subject to change. Raw data generated by some distribution
        self.data_X = np.load(self.args.root_path + '/' + self.args.numpy_path + '/nonlinear/' 
                              +self.graph.name + '-Gaussian-30000-' + str(self.graph.num_vertices) + '.npy')

        self.train_loader, self.test_loader = self.shuffle_data()
        self.fem_ground_truth()
        
        A_normalized = self.normalize_adj(self.graph.adjacency_matrix)

        Lap = torch.tensor(self.graph.laplacian).float()
        Lap = Lap.to_sparse()

        self.graph_info = [bc_value, interior_flag, A_normalized]
        # self.graph_info = [bc_value, interior_flag, Lap]


    def save_progress(self, np_data, L_inf, L_fro, milestone, counter):
        if L_inf < milestone[counter]:
            torch.save(self.model, self.args.root_path + '/' + self.args.model_path + '/model_' + str(counter))
            np.save(self.args.root_path + '/' + self.args.numpy_path + '/error_' + str(counter), np_data)
            counter += 1
        return counter

    def run(self):
        self.initialization()
 
        # self.model = MLP(self.args, self.graph_info)
        self.model = MixedNetwork(self.args, self.graph_info)

        # nn.init.constant_(self.model.encoder.weight.data, 0.)
        self.optimizer = optim.Adam(self.model.parameters(), lr=5*1e-4)
        # self.optimizer = optim.SGD(self.model.parameters(), lr=1e-4, momentum=0.7)

        milestone = [ 2**(-i) for i in range(-1, 11) ]
        for epoch in range(self.args.epochs):
            train_loss = self.train(epoch)
            # test_loss = self.test(epoch)  
            mean_L2_error = self.extra_test_fem(epoch)
            print('\n')

        torch.save(self.model, self.args.root_path + '/nonlinear/' + self.args.model_path + '/model')

    def extra_test_fem(self, epoch):
        self.model.eval()
        recon_batch = self.model(self.extra_test)
        mean_L2_error = ( (recon_batch - self.fem_solution)**2 * self.weight_area ).sum() / self.extra_test.shape[0]
        print('====> Mean L2 error: {:.6f}'.format(mean_L2_error))

        scalar_field_paraview(self.args, self.extra_test[epoch%self.extra_test.shape[0]], self.graph, "source")
        scalar_field_paraview(self.args, recon_batch[epoch%self.extra_test.shape[0]], self.graph, "nn")
        scalar_field_paraview(self.args, self.fem_solution[epoch%self.extra_test.shape[0]], self.graph, "fem")

        # if mean_L2_error < 0.0003: # 0.0002
        #     scalar_field_3D(recon_batch[epoch%self.extra_test.shape[0]].data.numpy(), self.graph)
        #     scalar_field_3D(self.fem_solution[epoch%self.extra_test.shape[0]].data.numpy(), self.graph)
        #     plt.show()

        return mean_L2_error


    def debug_ground_truth(self):
        gradient_x1_operator = torch.tensor(self.graph.gradient_x1).float()
        gradient_x2_operator = torch.tensor(self.graph.gradient_x2).float()
        boundary_operator = torch.tensor(self.graph.reset_matrix_boundary).float()
        interior_operator = torch.tensor(self.graph.reset_matrix_interior).float()

        A = -torch.matmul(gradient_x1_operator, gradient_x1_operator) \
            -torch.matmul(gradient_x2_operator, gradient_x2_operator)

        # If A is ill-conditioned, convergence will be painfully slow
        # Crude check about the condition number
        # print(np.max(A.data.numpy()), np.min(A.data.numpy()))

        A_inv = A.inverse()
        A_sp = A.to_sparse()
        return A, A_sp, A_inv

    def debug(self):
        self.initialization()

        # _, A_sm, _ = self.debug_ground_truth()
        A_sm = torch.tensor(self.graph.laplacian).float()

        def exact_u(x1, x2):
            return x1**2 + x2**2 + 0.3*np.random.standard_normal(x2.shape)*(1 - self.graph.boundary_flags)

        sol = get_graph_attributes(exact_u, self.graph)
        # sol = self.data_X[0]
        scalar_field_paraview(self.args, sol, self.graph, "sol")
        sol = torch.tensor(sol).float().view(-1, 1)

        lmbda = 0.001

        tmp1 = np.concatenate((lmbda*torch.matmul(A_sm, sol).data.numpy(), sol.data.numpy()), axis=1)
        tmp2 = np.expand_dims(self.graph.boundary_flags, axis=1)
        tmp = np.concatenate((tmp1, tmp2), axis=1) 
        # print(tmp)
        smooth = lmbda*torch.matmul(A_sm, sol) + sol
        for i in range(1000):
            smooth = lmbda*torch.matmul(A_sm, smooth) + smooth

        scalar_field_paraview(self.args, smooth, self.graph, "sm")

if __name__ == "__main__":
    args = arguments.args
    trainer = TrainerNonlinear(args)
    trainer.run()